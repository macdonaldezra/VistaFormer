{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Accuracy, MeanMetric, ClasswiseWrapper, ConfusionMatrix\n",
    "\n",
    "\n",
    "from vistaformer.config import get_model_config, PROJECT_ROOT\n",
    "from vistaformer.models import get_model\n",
    "from vistaformer.datasets.pastis.dataloader import (\n",
    "    get_val_transforms,\n",
    "    get_dataloader,\n",
    "    PASTISDataset,\n",
    ")\n",
    "from vistaformer.train_and_evaluate.eval_utils import generate_test_metrics\n",
    "\n",
    "\n",
    "num_params = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "root_path = Path(\"<very_real_path_to_training_outputs>\")\n",
    "assert root_path.exists(), f\"Path {root_path} does not exist\"\n",
    "\n",
    "config = get_model_config(root_path / \"config.yaml\")\n",
    "\n",
    "model = get_model(config)\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "print(f\"Number of trainable parameters: {num_params(model)}\")\n",
    "\n",
    "weights = torch.load(root_path / \"best_model.pth\", map_location=device)\n",
    "model.load_state_dict(weights[\"model_state_dict\"], strict=False)\n",
    "\n",
    "datapath = Path(\"<very_real_path_to_data>\")\n",
    "config.dataset.path = datapath\n",
    "out_path = Path(\"./outputs\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataloader = get_dataloader(config, fold=config.dataset.kwargs[\"test_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(\"very_real_path_to_training_outputs\")\n",
    "paths = [p.parent for p in list(root_path.rglob(\"*config.yaml\"))]\n",
    "\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=config.num_classes).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "print(f\"Computing confusion matrix for {len(paths)} models\")\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"Computing confusion matrix for {path.parent.parent.name}/{path.parent.name}...\")\n",
    "\n",
    "    config = get_model_config(path / \"config.yaml\")\n",
    "    config.dataset.path = datapath\n",
    "    dataloader = get_dataloader(config, fold=config.dataset.kwargs[\"test_folds\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            target = target.to(device)\n",
    "            if config.is_multi_input_model:\n",
    "                s2, s1a = data[\"s2\"].to(device), data[\"s1\"].to(device)\n",
    "                output = model(s2, s1a)\n",
    "                del data\n",
    "            else:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "\n",
    "            output = torch.argmax(output, dim=1)\n",
    "            confusion_matrix.update(output, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [c for c in list(dataloader.dataset.index_to_label.values()) if c != \"void-label\"]\n",
    "confusion_matrix_np = confusion_matrix.compute().cpu().numpy()\n",
    "conf_matrix = confusion_matrix_np[0:19, 0:19] # remove void label\n",
    "\n",
    "df_cm = pd.DataFrame(\n",
    "    conf_matrix / np.sum(conf_matrix, axis=1)[:, None],\n",
    "    index = [i for i in class_names],\n",
    "    columns = [i for i in class_names]\n",
    ").round(2)\n",
    "df_cm.to_csv(\"confusion_matrix.csv\")\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.set_theme(style='white', font_scale=0.7)\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"Blues\", fmt='g')\n",
    "plt.savefig(\"confusion_matrix.pdf\", facecolor=\"white\", transparent=False, dpi=100, bbox_inches=\"tight\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(config, fold=None)\n",
    "\n",
    "class_name_dict = {v: class_pixel_counts[k] for k, v in dataloader.dataset.index_to_label.items()}\n",
    "class_pixel_counts = defaultdict(int)\n",
    "total_pixels = 0\n",
    "\n",
    "for inputs, labels in dataloader:\n",
    "    labels = labels.squeeze()\n",
    "    # Count the pixels for each class\n",
    "    unique_classes, class_counts = torch.unique(labels, return_counts=True)\n",
    "    for class_idx, count in zip(unique_classes, class_counts):\n",
    "        class_pixel_counts[class_idx.item()] += count.item()\n",
    "        total_pixels += count.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "data = pd.DataFrame(class_name_dict.items(), columns=['Class', 'Frequency'])\n",
    "sns.barplot(x='Class', y='Frequency', data=data, hue='Class', palette=\"muted\")\n",
    "\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(list(class_name_dict.keys()))  # Ensure x-ticks are the class labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=-90)\n",
    "# Show the plot\n",
    "\n",
    "plt.savefig(\"pastis_class_labels.pdf\", facecolor=\"white\", dpi=100, bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vistaformer.datasets.mtlcc.dataloader import get_dataloader as get_mtlcc_dataloader\n",
    "\n",
    "class_pixel_counts = defaultdict(int)\n",
    "total_pixels = 0\n",
    "\n",
    "for split in [\"train\", \"test\", \"val\"]:\n",
    "    config.dataset.path = Path(\"/mnt/sata1/datasets/time_series/MTLCC/data_IJGI18/datasets/full/240pkl\")\n",
    "    dataloader = get_mtlcc_dataloader(config, split=split)\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        labels = labels.squeeze()\n",
    "        # Count the pixels for each class\n",
    "        unique_classes, class_counts = torch.unique(labels, return_counts=True)\n",
    "        for class_idx, count in zip(unique_classes, class_counts):\n",
    "            class_pixel_counts[class_idx.item()] += count.item()\n",
    "            total_pixels += count.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bar chart\n",
    "class_name_dict = {v: class_pixel_counts[k] for k, v in dataloader.dataset.index_to_label.items()}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "data = pd.DataFrame(class_name_dict.items(), columns=['Class', 'Frequency'])\n",
    "sns.barplot(x='Class', y='Frequency', data=data, hue='Class', palette=\"muted\")\n",
    "sns.set_theme(style='white', font_scale=0.7)\n",
    "# plt.bar(list(class_name_dict.keys()), list(class_name_dict.values()), color='blue')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of PASTIS Class Labels')\n",
    "plt.xticks(list(class_name_dict.keys()))  # Ensure x-ticks are the class labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=-90)\n",
    "# Show the plot\n",
    "\n",
    "plt.savefig(\"mtlcc_class_labels.pdf\", facecolor=\"white\", dpi=100, bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from itertools import zip_longest\n",
    "from collections import Counter\n",
    "from numpy import prod\n",
    "from functools import partial\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from fvcore.nn.jit_handles import generic_activation_jit\n",
    "from natten.flops import add_natten_handle\n",
    "\n",
    "\n",
    "def get_shape(val: object) -> typing.List[int]:\n",
    "    \"\"\"\n",
    "    Get the shapes from a jit value object\n",
    "    \"\"\"\n",
    "    if val.isCompleteTensor():\n",
    "        r = val.type().sizes()\n",
    "        if not r:\n",
    "            r = [1]\n",
    "        return r\n",
    "    elif val.type().kind() in (\"IntType\", \"FloatType\"):\n",
    "        return [1]\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "def basic_binary_op_flop_jit(inputs, outputs, name):\n",
    "    input_shapes = [get_shape(v) for v in inputs]\n",
    "    # for broadcasting\n",
    "    input_shapes = [s[::-1] for s in input_shapes]\n",
    "    max_shape = np.array(list(zip_longest(*input_shapes, fillvalue=1))).max(1)\n",
    "    flop = prod(max_shape)\n",
    "    flop_counter = Counter({name: flop})\n",
    "    return flop_counter\n",
    "\n",
    "\n",
    "def pretty_flops(num_flops: int):\n",
    "    \"\"\"\n",
    "    Pretty print the number of FLOPs.\n",
    "    \"\"\"\n",
    "    units = [(\"GFLOPs\", 1e9), (\"MFLOPs\", 1e6), (\"KFLOPs\", 1e3), (\"FLOPs\", 1)]\n",
    "    for unit_name, unit_value in units:\n",
    "        if num_flops >= unit_value:\n",
    "            return f\"{num_flops / unit_value:.2f} {unit_name}\"\n",
    "\n",
    "    return \"0 FLOPs\"\n",
    "\n",
    "\n",
    "input_dim = 32\n",
    "config.image_size = input_dim\n",
    "config.max_seq_len = 30\n",
    "config.model_kwargs[\"seq_lens\"] = [30, 15, 7]\n",
    "\n",
    "model = get_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "inputs = torch.randn(4, 30, 6, input_dim, input_dim).to(device)\n",
    "sen = torch.randn(4, 30, 10, input_dim, input_dim).to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "counter = FlopCountAnalysis(model, inputs=(sen, inputs))\n",
    "counter.set_op_handle(\"aten::softmax\", generic_activation_jit(\"aten::softmax\"))\n",
    "counter.set_op_handle(\"aten::sigmoid\", generic_activation_jit(\"aten::gelu\"))\n",
    "counter.set_op_handle(\"aten::gelu\", generic_activation_jit(\"aten::gelu\"))\n",
    "counter.set_op_handle(\"aten::mish\", generic_activation_jit(\"aten::mish\"))\n",
    "counter.set_op_handle(\"aten::div_\", partial(basic_binary_op_flop_jit, name='aten::div_'))\n",
    "counter.set_op_handle(\"aten::mul\", partial(basic_binary_op_flop_jit, name='aten::mul'))\n",
    "counter.set_op_handle(\"aten::add\", partial(basic_binary_op_flop_jit, name='aten::add'))\n",
    "counter.set_op_handle(\"aten::add_\", partial(basic_binary_op_flop_jit, name='aten::add_'))\n",
    "add_natten_handle(counter)\n",
    "\n",
    "print(f\"Total number of estimated flops: {pretty_flops(counter.total())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data provided\n",
    "model_flops = {\n",
    "    \"U-TAE\": [11.64, 46.81, 105.32, 187.24],\n",
    "    \"TSViT\": [65.23, 326.36, 979.64, 2352.28],\n",
    "    \"VistaFormer\": [7.58, 26.8, 113.79, 326.24],\n",
    "    \"VistaFormer(NeighbourAttn)\": [4.85, 14.97, 32.57, 57.67],\n",
    "}\n",
    "\n",
    "# Input dimensions\n",
    "input_dimensions = [32, 64, 96, 128]\n",
    "\n",
    "# Convert the data into a format suitable for Seaborn\n",
    "data = {\n",
    "    \"Input Dimensions\": input_dimensions * len(model_flops),\n",
    "    \"GFLOPs\": sum(model_flops.values(), []),\n",
    "    \"Model\": [model for model in model_flops for _ in input_dimensions]\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=\"Input Dimensions\", y=\"GFLOPs\", hue=\"Model\", data=data, palette=\"muted\", marker='o')\n",
    "sns.set_theme(style='white', font_scale=0.7)\n",
    "# Add titles and labels\n",
    "# plt.title('GFLOPs vs Input Dimensions for Different Models')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Input Dimensions', fontweight='bold', fontsize=10)\n",
    "plt.ylabel('GFLOPs (log scale)', fontweight=\"bold\", fontsize=10)\n",
    "\n",
    "plt.savefig(\"model_gflops.pdf\", facecolor=\"white\", transparent=False, dpi=100, bbox_inches=\"tight\", format=\"pdf\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vistaformer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
